\chapter{Implementation Details}

\section{Development Methodology}
\subsection{Iterative Development Process}
CodeOrient was developed using an iterative approach to continuously refine and integrate feedback. The key stages of the development process are outlined below:
\begin{enumerate}
    \item \textbf{Requirement Analysis:} The pain points of new developers navigating unfamiliar codebases were identified to gather initial requirements.
    \item \textbf{Prototyping:} To validate core concepts, early prototypes of the search and Generative UI systems were built. For example, a simple weather card UI was created to test the Generative UI's capabilities is shown in Figure \ref{fig:dynamic_card_generation_weather_app}.
    \begin{figure}[H]
        \centering
        {\small
        \includegraphics[width=5.5in]{fig/fig8.png}
        \caption{\textit{Prototype of Dynamic Card Generation for Weather App}}
        \label{fig:dynamic_card_generation_weather_app}
        }
        \end{figure}
        \item \textbf{Incremental Development:} The search module, code graph analysis, and RAG pipeline were identified as priority features. Developement for these features was done incrementally to gather feedback early.
    \item \textbf{Integrating User Feedback:} Feedback from user testing was regularly incorporated to improve the user experience of CodeOrient.
    \item \textbf{Final Testing and Optimisation:} The system underwent rigorous testing and automated deployment via CI/CD pipelines to ensure performance and reliability of production release.
\end{enumerate}

\subsection{Version Control and Branching Strategy}
This project utilised Git and GitHub for version control. Feature-branching was used to isolate the development of core features. This ensured that the main branch remained stable and protected for user testing. As for deployment, it was automated through CI/CD pipelines and was deployed to Vercel. Figure \ref{fig:git_branching_strategy} illustrates the branching strategy which squashed feature branches into the main branch after code reviews and testing.
\begin{figure}[H]
    \centering
    {\small
    \includegraphics[width=5.5in]{fig/fig9.png}
    \caption{\textit{Git Branching Strategy for CodeOrient Development}}
    \label{fig:git_branching_strategy}
    }
\end{figure}

\section{Core Implementation Components}
\subsection{Search Module}
\subsubsection{BM25 Sparse Retrieval}
To handle keyword-based searches, the system implements the Best Matching 25 (BM25) algorithm. The BM25 score for a document \( D \) given a query \( Q \) is computed as:
\[
\text{BM25}(D, Q) = \sum_{q_i \in Q} IDF(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{avgdl})}
\]
where:
\begin{itemize}
    \item \( f(q_i, D) \) is the term frequency of query term \( q_i \) in document \( D \).
    \item \( |D| \) is the length of document \( D \).
    \item \( avgdl \) is the average document length in the corpus.
    \item \( k_1 \) and \( b \) are hyperparameters, typically set to \( k_1 = 1.5 \) and \( b = 0.75 \) for general text.
    \item \( IDF(q_i) \) is the inverse document frequency of term \( q_i \), which is calculated as:
    \[
    IDF(q_i) = \log \frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}
    \]
    where \( N \) is the total number of documents and \( n(q_i) \) is the number of documents containing term \( q_i \).
\end{itemize}
This retrieval method is used by CodeOrient to retrieve code snippets that match the keywords in the user query. For example, if a user searchs for an exact function name, BM25 will prioritise documents containing that exact term.

\subsubsection{Dense Embedding Retrieval}
\texttt{bge-large-en-v1.5} embedding model is used for dense retrieval. Each chunk of code is converted to a 1024-dimensional vector and stored in Upstash Vector. This allows the engine to retrieve code snippets based on semantic similarity to the user query. The similarity between the query vector \( Q \) and document vector \( D \) is computed using cosine similarity:
\[
\text{cosine\_similarity}(\vec{a}, \vec{b}) = \frac{\vec{a} \cdot \vec{b}}{\|\vec{a}\| \|\vec{b}\|}
\]

\subsubsection{Hybrid Search Integration}
The final ranking is acheived through Distributed-Based Score Fusion. The normalised score is computed as:
\[
Score = \frac{s - (\mu - 3\sigma)}{(\mu + 3\sigma) - (\mu - 3\sigma)}
\]
where:
\begin{itemize}
    \item $s$ is the score.
    \item $\mu$ is the mean of the scores.
    \item $\sigma$ is the standard deviation.
    \item $(\mu - 3\sigma)$ represents the minimum value (lower tail of the distribution).
    \item $(\mu + 3\sigma)$ represents the maximum value (upper tail of the distribution).
\end{itemize}
This approach takes into account the distribution of scores which is more sensitive to variation in score ranges from the different retrieval methods.

\subsection{Code Graph Analysis}
This section details the transformation of the code chunks into an interactive graph.
\subsubsection{Dependency Extraction}
Rather than using AST for traversal, the system utilise a recursive language aware splitting strategy. This approach splits the code into smaller chunks while maintaining the programming language's nuances, syntax and structure. This strategy is outlined in Algorithm \ref{alg:recursive_splitting}.

\begin{algorithm}[H]
\SetAlgoLined
\DontPrintSemicolon
\KwIn{File Content $C$, File Extension $E$, ChunkSize $S$, Overlap $O$}
\KwOut{List of Semantic Chunks $K$}

$K \gets \emptyset$\;
$Language \gets \text{MapExtensionToLanguage}(E)$\;

\If{$Language$ is supported}{
    $Splitter \gets \text{InitialiseLangChainSplitter}(Language, S, O)$\;
}
\Else{
    $Separators \gets \{ \texttt{"\textbackslash nclass "}, \texttt{"\textbackslash ndef "}, \texttt{"\textbackslash n\textbackslash n"}, \texttt{"\textbackslash n"}, \texttt{" "} \}$\;
    $Splitter \gets \text{InitialiseRecursiveSplitter}(Separators, S, O)$\;
}

$Documents \gets Splitter.\text{splitText}(C)$\;

\ForEach{$Doc$ in $Documents$}{
    $Chunk \gets \text{ExtractContentAndMetadata}(Doc)$\;
    Add $Chunk$ to $K$\;
}

\Return{$K$}\;
\caption{Language-Aware Recursive Code Splitting Algorithm}
\label{alg:recursive_splitting}
\end{algorithm}

\subsubsection{Graph Construction Algorithm}
As React Flow requires a structured object to render the graph, the extracted entities and their relationships are mapped to a JSON object. In Listing \ref{lst:graph_metadata_schema}, the code entities are represented as nodes and their relationships as edges. 
\begin{listing}[H]
\begin{minted}[frame=lines, framesep=2mm, fontsize=\footnotesize, linenos]{typescript}
/**
 * Schema for Code Graph Nodes
 */
export type CodeGraphNode = {
  id: string; // Unique identifier: userId::repo::path::type::name
  label: string; // The display name of the entity
  type?: "file" | "function" | "class" | "component"; 
  filePath?: string; // Original source file path
  codeSnippet?: string; // The raw source code associated with the entity
  description?: string; // Extracted docstring or JSDoc comment
};

/**
 * Schema for Code Graph Edges
 */
export type CodeGraphEdge = {
  id: string; // Composite ID: sourceID->targetID
  source: string; // ID of the originating node
  target: string; // ID of the destination node
  label?: string; // Relationship type
  type?: "imports" | "calls" | "extends" | "uses";
  animated?: boolean; // Visual indicator for logic flow
};
\end{minted}
\caption{TypeScript interfaces for CodeOrient graph entities.}
\label{lst:graph_metadata_schema}
\end{listing}

\subsection{Generative UI System}
The Generative UI system dynamically creates interactive UI cards based on user queries. The implementation involves several key components:
\subsubsection{Toolkit Selection}
External tools are integrated to enhance the LLM's capabilities. The available tools for selection are:
\begin{itemize}
    \item \textbf{Code Graph Tool:} Retrieves and visualises code entities and their relationships.
    \item \textbf{Repository Search Tool:} Fetches all repositories associated with the user.
    \item \textbf{GitHub Search Tool:} Fetches specific files or code snippets from GitHub directly.
    \item \textbf{Vector Search Tool:} Interacts with the hybrid search module to fetch relevant code snippets.
\end{itemize}

\subsubsection{Dynamic Card Generation}
Based on the user query, the LLM intelligently selects the appropriate tool(s) to fulfill the request. It then generates a structured JSON object based on the tool(s) chosen which results in different card visualisations. It is dynamically streamed to the frontend for real-time rendering. The different visualisations supported are:
\begin{itemize}
    \item \textbf{Repository Card} 
    \begin{figure}[H]
        \centering
        {\small
        \includegraphics[width=5.5in]{fig/fig10.png}
        \caption{\textit{Example of Repository Card in Generative UI}}
        \label{fig:repository_card_example}
        }
    \end{figure}
    \item \textbf{Code Graph Card} 
    \begin{figure}[H]
        \centering
        {\small
        \includegraphics[width=5.5in]{fig/fig11.png}
        \caption{\textit{Example of Code Graph Card in Generative UI}}
        \label{fig:code_graph_card_example}
        }
    \end{figure}
\end{itemize}

\subsection{Large Language Model Integration}
CodeOrient is model agnostic and can integrate with any LLM providers that support tool-calling. 
\subsubsection{Model Selection and Justification}
In this implementation, CodeOrient utilises OpenAI's GPT-4.1-mini model due to its advanced reasoning capabilities and large context window, which prevents "lost in the middle" degradation. Furthermore, the latency and cost-effectiveness of the mini variant make it suitable for real-time applications compared to State-Of-The-Art reasoning models.
\subsubsection{Prompt Engineering Strategies}
To optimise the performance of the LLM and reduce hallucinations during code exploration, a Multi-layered Prompting Strategy is employed with specialised personas for different stages of the RAG pipeline:
\begin{itemize}
    \item \textbf{Search Architect Persona:} This persona's responsibility is to decompose a user's query into a structured search plan. The plan entails the steps it will take before generating a graph. The most important responsibility is to break down complex queries into non-overlapping sub-queries that target different parts of the codebase. Additionally, the repository's tree is provided as context to guide the planning process. This reduces the hallucination of non-existent files or functions. In Figure \ref{fig:planning_example}, the step by step breakdown of the planning process is illustrated to the user.
    \begin{figure}[H]
        \centering
        {\small
        \includegraphics[width=5.5in]{fig/fig12.png}
        \caption{\textit{Example of Multistep Planning by Search Architect Persona}}
        \label{fig:planning_example}
        }
    \end{figure}
    \item \textbf{Gap Analyser Persona:} To give the LLM autonomy in exploring the codebase, this persona acts as a quality control layer to identify gaps in the retreived context. An additional search iteration with refined queries will be triggered if the context is deemed insufficient. This iterative process continues until the LLM can answer the user's query accurately. Figure \ref{fig:gap_analysis_example} illustrates an example of the LLM identifying a need for additional context.
    \begin{figure}[H]
        \centering
        {\small
        \includegraphics[width=5.5in]{fig/fig13.png}
        \caption{\textit{Example of Gap Analysis by Gap Analyser Persona}}
        \label{fig:gap_analysis_example}
        }
    \end{figure}
    \item \textbf{Graph Architect Persona:} As React Flow requires a structured object to render the graph, this persona translates the retrieved context into a graph object. This is achieved by identifying the relevant entities and their relationships, before formatting them into nodes and edges. 
\end{itemize}

\subsection{RAG Pipeline Implementation}
This pipeline iteratively retrieves relevant code snippets to ground the LLM's responses. The key components are outlined below:

\subsubsection{Query Processing}
User queries are usually ambiguous and may include typos. To address this, the pipeline first rephrases the query into technical search queries that align with the codebase's terminology and structure. This is achieved using the Search Architect persona described earlier.

\subsubsection{Multistep Planning \& Exploration}
Mentioned previously, the Search Architect persona decomposes complex queries and conducts a breadth-first exploration. 

\subsubsection{Retrieval \& Ranking}
The refined queries are used to fetch relevant code chunks from the hybrid search module, with \texttt{userId} and \texttt{repoFullName} used as filters to ensure strict multi-tenancy. K-Nearest Neighbour with $K=10$ is used to retrieve the most similar chunks based on cosine similarity.
The retrieved chunks are then ranked using the Distributed-Based Score Fusion method to ensure the most relevant snippets are prioritised.

\subsubsection{Context Assembly}
The top-ranked code chunks are assembled and wrapped in XML-style tags containing metadata for the LLM to accurately reference the sources during the response phase. An example of the assembled context is shown in Listing \ref{lst:assembled_context_example}.
\begin{listing}[H]
\begin{minted}[frame=lines, framesep=2mm, fontsize=\footnotesize, linenos]{xml}
<chunk file="lib/auth.ts" lines="12-45" url="...">
  [Code Snippet]
</chunk>
\end{minted}
\caption{Example of Assembled Context with Source Metadata.}
\label{lst:assembled_context_example}
\end{listing}

\subsubsection{Citation Extraction and Grounding}
To increase the credibility of the generated answer, all sources retrieved from the RAG pipeline are shown to the user in Figure~\ref{fig:rag_sources_example}. To further reduce hallucination in the response, the LLM is prompted to provide inline citations using markdown format \texttt{[file\_path](link\_to\_source\_code)}. The frontend parses these citations to create clickable links that direct users to the exact source code locations. An example of inline citations is shown in Figure~\ref{fig:inline_citations_example}.

\begin{figure}[H]
    \centering
    {\small
    \includegraphics[width=5.5in]{fig/fig14.png}
    \caption{\textit{Example of sources retrieved from RAG pipeline}}
    \label{fig:rag_sources_example}
    }
\end{figure}

\begin{figure}[H]
    \centering
    {\small
    \includegraphics[width=5.5in]{fig/fig15.png}
    \caption{\textit{Example of inline citations in LLM response}}
    \label{fig:inline_citations_example}
    }
\end{figure}

\section{Database Schema}
CodeOrient utilises PostgreSQL for relational data storage, managed via Prisma ORM. The schema is organised into three primary clusters:
\begin{itemize}
    \item \textbf{User Identity \& Session:} Tables to manage user authentication and GitHub Personal Access Tokens (PATs).
    \item \textbf{Conversation State:} Tables storing \texttt{Chat}, \texttt{Message}, and \texttt{Part} models to support Generative UI and tool-calling outputs.
    \item \textbf{Indexing Lifecycle:} Table to track the asynchronous indexing jobs for user repositories, used in the RAG pipeline.
\end{itemize}
The complete Prisma schema is provided in Listing \ref{lst:database_schema}.

\begin{minted}[
    frame=lines, 
    framesep=2mm, 
    fontsize=\footnotesize, 
    linenos,
    breaklines,      % Allows breaking of long lines
    breakanywhere    % Allows breaking mid-word if necessary
]{text}
// Core User and Repository Models
model User {
  id            String    @id
  name          String
  email         String    @unique
  githubPAT     String?   // Encrypted token for repository access
  chats         Chat[]
  createdAt     DateTime  @default(now())
  @@map("user")
}

model IndexedRepository {
  id            String         @id @default(cuid())
  userId        String
  repoFullName  String         // Format: "owner/repo"
  status        IndexingStatus @default(PENDING)
  progress      Int            @default(0)
  totalFiles    Int            @default(0)
  indexedFiles  Int            @default(0)
  lastIndexedAt DateTime?
  
  @@unique([userId, repoFullName])
  @@map("indexed_repository")
}

// Conversational State with Generative UI Support
model Chat {
  id        String    @id @default(cuid())
  title     String
  messages  Message[]
  userId    String?
  User      User?     @relation(fields: [userId], references: [id])
  @@map("chat")
}

model Message {
  id        String      @id @default(cuid())
  chatId    String
  chat      Chat        @relation(fields: [chatId], references: [id], onDelete: Cascade)
  role      MessageRole
  parts     Part[]      // Supports multi-modal and tool-call outputs
  @@map("message")
}

model Part {
  id        String          @id @default(cuid())
  messageId String
  message   Message         @relation(fields: [messageId], references: [id], onDelete: Cascade)
  type      MessagePartType
  
  // Generative UI and Tool Metadata
  tool_toolCallId           String?
  tool_visualiseCodeGraph_output Json? // Stores React Flow graph data
  data_codeGraph            Json?
  
  @@map("part")
}

enum IndexingStatus {
  PENDING
  CLONING
  PARSING
  INDEXING
  COMPLETED
  FAILED
}
\end{minted}
\begin{center}
    \captionof{listing}{Prisma Database Schema.}
    \label{lst:database_schema}
\end{center}

\section{UI/UX Enhancements}
This section highlights the various UI/UX features implemented to enhance user experience in CodeOrient.
\subsection{Account Dashboard}
The account dashboard allows users to monitor their interaction with the platform. As shown in Figure \ref{fig:account_dashboard}, users can view statistics such as:
\begin{itemize}
    \item Recent Activities
    \item Usage statistics (e.g., lifetime searches, total repositories analysed)
\end{itemize}

\begin{figure}[H]
    \centering
    {\small
    \includegraphics[width=5.5in]{fig/fig16.png}
    \caption{\textit{Example of Account Dashboard}}
    \label{fig:account_dashboard}
    }
\end{figure}

\subsection{Sharing of Conversations}
To facilitate collaboration, CodeOrient supports the sharing of conversations via unique read-only links. Figure \ref{fig:sharing_conversation_link} illustrates the shared chat view where authorised users can view the conversation history in a read-only format.
\begin{figure}[H]
    \centering
    {\small
    \includegraphics[width=5.5in]{fig/fig17.png}
    \caption{\textit{Example of Sharing Conversation Link}}
    \label{fig:sharing_conversation_link}
    }
\end{figure}

\subsection{Setting Preferences}
This interface allows users to customise their experience. As illustrated in Figure \ref{fig:user_preferences_settings}, this tab of the settings page focuses on data integration. Users can securely connect their GitHub accounts via Personal Access Tokens (PATs) to enable repository indexing and analysis. Furthermore, users can index any public or private repositories they have access to. The entire indexing process is asynchronous, and real-time progress updates are provided. By enabling indexing, the LLM will prioritise searching the vector database over GitHub to reduce latency and cost.
\begin{figure}[H]
    \centering
    {\small
    \includegraphics[width=5.5in]{fig/fig18.png}
    \caption{\textit{Example of User Preferences Settings}}
    \label{fig:user_preferences_settings}
    }
\end{figure}

\section{Challenges and Solutions}
\subsection{Handling Large Codebases}
As GitHub API has strict rate limits, retrieving large codebases can be time-consuming. To address this challenge, the system implements a \textbf{Streaming Tarball Strategy}. Instead of making multiple API calls, the system fetches a single compressed \texttt{.tar.gz} archive, reducing network calls by over 90\% and processing repository data entirely in-memory.

\subsection{Real-Time Graph Updates}
LLMs can take several seconds to generate responses, which can lead to a suboptimal user experience. Hence, Server-Sent Events (SSE) is used to stream the graph data incrementally which allows React Flow to render an empty canvas initially and progressively add nodes and edges after the generation has completed.
