\chapter{Results and Discussions}

\section{Quantitative Results}
The evaluation of CodeOrient was conducted across 30 queries spanning three modern and well-maintained repositories. The metrics and results across CodeOrient, the ablation model, and the GitHub Search baseline are summarised in Table~\ref{tab:evaluation_results}.

\begin{table}[htbp]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{CodeOrient (Full RAG)} & \textbf{GitHub + Agent (Ablation)} & \textbf{GitHub Search (Baseline)} \\ 
\midrule
Recall@System   & 92.50\%                        & 66.39\%                            & 2.78\%                            \\
Success@Turn    & 1.50                           & 2.31                               & -                                 \\
Latency         & 62.79s                         & 77.83s                             & -                                 \\
Latency (Cache) & 4.86s                          & 5.03s                              & -                                 \\
Faithfulness    & 98.43\%                        & 98.33\%                            & -                                 \\
Graph Recall    & 93.61\%                        & 69.72\%                            & -                                 \\
CTD             & 0.07                           & 0.03                               & 5.27                              \\ 
\bottomrule
\end{tabular}
}
\caption{Average Evaluation Metrics Across 30 Queries}
\label{tab:evaluation_results}
\end{table}

\section{Comparative Analysis with Baselines}
\subsection{Performance vs. GitHub Search}
Comparing CodeOrient to the GitHub Search baseline, there is a significant improvement in both Recall@System (92.50\% vs. 2.78\%) and Clicks to Discovery (0.07 vs. 5.27). This showcases the limitations of traditional keyword-based search in understanding natural language queries. Furthermore, the baseline's low recall rate and high CTD indicate that developers would need to manually sift through search results, which is inefficient and time consuming. On the contrary, CodeOrient was able to retrieve the required code snippets with a high recall rate with minimal user interaction which demonstrates its effectiveness in enhancing developer productivity. 
\subsection{Performance vs. Ablation Model}
Comparing CodeOrient to the ablation model, which uses the same search mechanism but swapping the RAG pipeline with GitHub's Search API directly, there is a significant improvement in Recall@System (92.50\% vs. 66.39\%), Success@Turn (1.50 vs. 2.31), Graph Recall (93.61\% vs. 69.72\%) and Latency (62.79s vs. 77.83s). 

Firstly, it highlights the importance of the RAG pipeline in effectively retrieving relevant code snippets and generating accurate graph visualisations. Secondly, the improvement in Success@Turn indicates that CodeOrient is more efficient in retrieving the correct code files within a fewer number of turns which ties directly to the improved latency. 

Thirdly, the similar faithfulness scores between the two models suggest that the multi-layered prompting strategy in CodeOrient successfully mitigates most hallucination issues, and the main performance gains are attributed to the enhanced retrieval and graph generation capabilities provided by the RAG pipeline. Lastly, the use of caching in CodeOrient reduced the latency of repeated queries by an average of 92.2\% and 93.5\% for CodeOrient and the ablation model respectively, which demonstrates the efficiency of caching in improving response times for repeated queries.

\section{Key Findings}
\subsection{Impact of Graph Visualisation and Emergent Inference}
With a Recall@System of 92.50\%, CodeOrient successfully bridges the gap between code retrieval and architectural understanding by generating accurate graph visualisations that capture the relationships between code snippets.

Instances of ``Emergent Inference" was observed, where the system identified imported relationships that were not explicitly retrieved, allowing the system to maintain an accurate graph visualisation even when some relevant files were missed in the initial retrieval step. This is evident where the average Graph Recall (93.61\%) is higher than the average Recall@System (92.50\%), which suggests that the system was able to infer some relationships between code snippets that were not directly retrieved.

\subsection{Effectiveness of Citation Grounding}
The high Faithfulness score of 98.43\% is a direct result of the citation grounding mechanism. Prompting the LLM to provide inline citations when generating responses ensures that the information provided is directly traceable to specific code snippets, which significantly reduces the likelihood of hallucination.
\section{Case Studies}
Across the 30 queries, there were several interesting cases that highlighted the strengths and limitations of CodeOrient.
\subsection{Case Study 1: ``How does the system resolve and inject dependencies?"}
\label{Case_Study_1}
CodeOrient only managed to achieve a Recall@System of 50\% for this complex architectural query. While CodeOrient managed to locate the primary dependency utility file in (\texttt{utils.py}), it was observed that it failed to retrieve the underlying implementation. 

This highlights a classic ``multi-hop" challenge in code retrieval, where the recursive nature of dependencies are spread across multiple files. This suggests a gap in the system's ability to effectively traverse deeper levels of dependencies, which is crucial for queries that require understanding the full execution flow of a codebase.

This directly impacts the graph recall performance, where the system achieved a graph recall of 50\% for this query. This indicates that while the primary nodes were retrieved, some of the secondary nodes (e.g., files containing the underlying implementation) were missed. This case study highlights an area for future improvement in CodeOrient, which is to enhance the multi-hop retrieval capabilities to ensure that all relevant files across multiple levels of dependencies are retrieved effectively.

\subsection{Case Study 2: ``How are the button visual variants defined?"}
This query demonstrates a scenario where even though the system only achieved a Recall@System of 50\%, the generated graph visualisation achieved a graph recall of 100\%. This showcases emergent inference capabilities of the LLM when it comes to understanding import relationships in the code snippets. 

In this case, CodeOrient successfully retrieved the main file defining the button variants (\texttt{button.tsx}), and through the citation grounding and multi-layered prompting, it was able to infer the relevant related file that were not directly retrieved (\texttt{utils.ts}). This is shown in Figure \ref{fig:case_study_2}, where the inferred node is highlighted in red.

This highlights a unique strength of CodeOrient, which is its ability to leverage the reasoning capabilities of LLMs to fill in gaps in retrieval and generate more complete graph visualisations, even when some relevant files are missed in the initial retrieval step. This also suggests that while improving recall is important, enhancing the reasoning and inference capabilities of the system can also significantly contribute to better performance in terms of graph recall and overall user experience.

\begin{figure}[H]
    \centering
    {\small
    \includegraphics[width=5.5in]{fig/fig19.png}
    \caption{\textit{Graph visualisation for Case Study 2}}
    \label{fig:case_study_2}
    }
\end{figure}

\subsection{Case Study 3: ``How is dark mode theme switching implemented?"}
This query serves as a justification for the use of LLM Evaluation Loop in the system. This query required identifying three specific files: \texttt{active-theme.ts}, \texttt{theme-selector.tsx} and \texttt{mode-switcher.tsx}.

CodeOrient achieved a perfect Recall@System of 100\% for this query with a Success@Turn of 3-2-1. This meant that the system refined its search across three turns which led to the successful retrieval of all three files. In contrast, the ablation model stalled at a Recall@System of 67\%. 

Hence, this example highlights the system's ability to identify missing gaps based on the initial retrieval results, and then refining its search queries in the follow-up turns. This resulted in a complete graph visualisation as shown in Figure \ref{fig:case_study_3}, which accurately captured the relationships between the three files.

\begin{figure}[H]
    \centering
    {\small
    \includegraphics[width=5.5in]{fig/fig20.png}
    \caption{\textit{Graph visualisation for Case Study 3}}
    \label{fig:case_study_3}
    }       
\end{figure}

\section{Implications for Developer Onboarding}
The results of this evaluation showcased a reduction of CTD (Clicks to Discovery) from 5.27 with the GitHub Search baseline to 0.07 with CodeOrient. This significant reduction in CTD demonstrates how CodeOrient effectively eliminates the need for developers to manually sift through search results, which is a common pain point in traditional code search. A lower CTD suggests that deveopers can spend more time understanding and working with the code rather than searching for it, which can significantly enhance productivity and reduce the time taken for onboarding.

In addition, the high graph recall and faithfulness scores demonstrate that CodeOrient can provide accurate graph visualisations to assist new developers in understanding the relationships between different components in the codebase. This can be particularly beneficial for onboarding, as it allows new developers to quickly grasp the overall structure of the codebase and how the different components interact with each other. 
\section{Limitations}
\subsection{Technical Limitations}
\subsubsection{Multi-Hop Retrieval Challenge}
The ``multi-hop" retrieval mentioned in \autoref{Case_Study_1} showcases how the system struggles with ``multi-hop" dependency retrieval. As the system is currently limited to a maximum of 3 turns in the evaluation loop, it may not be able to retrieve all relevant files. This is a common tradeoff between latency and recall, as allowing for more turns could potentially improve recall rate but would also increase latency.

\subsubsection{Latency}
Without caching, the average latency of the system is rather high, at 62.79s. The RAG pipeline and Evaluation Loop which requires multiple calls to the LLM and vector database, are the main contributors to this latency. Although caching strategies have been implemented to reduce latency for repeated queries, the cold start latency remains a concern for user experience. One solution implemented was to stream the response as it is generated, reducing the perceived latency for users.

\subsubsection{Context Window}
Current LLMs have a limited context window, which can pose challenges for queries that require retrieving a large number of relevant files. If the retrieved code snippets exceed the context window, it may lead to incomplete or inaccurate responses.

\subsection{Threats to Validity}
\subsubsection{Internal Validity} 
As LLMs are inherently stochastic, there is a possibility that the results may vary across different runs of the evaluation. This could potentially affect the internal validity of the evaluation. To mitigate this, a temperature of 0.0 was used for all LLM calls to ensure a more deterministic output.

Additionally, it is important to acknowledge that the evaluation of faithfulness and graph recall involves some level of subjectivity. To address this, clear guidelines and criteria were established for scoring these metrics to ensure consistency and reduce bias. 
\subsubsection{External Validity}
The evaluation was conducted on a golden set of 30 queries across three modern and well-maintained repositories. The queries were designed to be representative of common developer information needs, but they may not cover the full spectrum of queries that developers may have in real-world scenarios. Additionally, the repositories chosen for evaluation may not be representative of all types of codebases, such as legacy codebases or those with less documentation. Therefore, the generalisability of the results to other queries and repositories may be limited.

\subsubsection{Construct Validity}
While Clicks to Discovery (CTD) measures the speed of finding information, it does not necessarily measure the depth of a developer's understanding. However, the high faithfulness and graph recall scores suggest that the information provided by CodeOrient is not only quickly discoverable but also accurate and comprehensive, which can contribute to a deeper understanding of the codebase.




