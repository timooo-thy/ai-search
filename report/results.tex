\chapter{Results and Discussions}

\section{Quantitative Results}
The quantitative evaluation results are summarised in Table~\ref{tab:evaluation_results}, which presents the average metrics across all 30 queries for CodeOrient, the ablation model, and the GitHub Search baseline.

\begin{table}[htbp]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{CodeOrient (Full RAG)} & \textbf{GitHub + Agent (Ablation)} & \textbf{GitHub Search (Baseline)} \\ 
\midrule
Recall@System   & 92.50\%                        & 66.39\%                            & 2.78\%                            \\
Success@Turn    & 1.50                           & 2.31                               & -                                 \\
Latency         & 62.79s                         & 77.83s                             & -                                 \\
Latency (Cache) & 4.86s                          & 5.03s                              & -                                 \\
Faithfulness    & 98.43\%                        & 98.33\%                            & -                                 \\
Graph Recall    & 93.61\%                        & 69.72\%                            & -                                 \\
CTD             & 0.07                           & 0.03                               & 5.27                              \\ 
\bottomrule
\end{tabular}
}
\caption{Average Evaluation Metrics Across 30 Queries}
\label{tab:evaluation_results}
\end{table}

\section{Comparative Analysis with Baselines}
\subsection{Performance vs. GitHub Search}
Comparing CodeOrient to the GitHub Search baseline, there is a significant improvement in both Recall@System (92.50\% vs. 2.78\%) and Clicks to Discovery (0.07 vs. 5.27). This showcases the limitations of traditional keyword-based search in understanding natural language queries. Furthermore, the baseline's low recall rate and high CTD indicate that developers would need to manually sift through search results, which is inefficient and time consuming. On the contrary, CodeOrient was able to retrieve the required code snippets with a high recall rate with minimal user interaction which demonstrates its effectiveness in enhancing developer productivity. 
\subsection{Performance vs. Ablation Model}
Comparing CodeOrient to the ablation model, which uses the same search mechanism but swapping the RAG pipeline with GitHub's Search API directly, there is a significant improvement in Recall@System (92.50\% vs. 66.39\%), Success@Turn (1.50 vs. 2.31), Graph Recall (93.61\% vs. 69.72\%) and Latency (62.79s vs. 77.83s). 

Firstly, it highlights the importance of the RAG pipeline in effectively retrieving relevant code snippets and generating accurate graph visualisations. Secondly, the improvement in Success@Turn indicates that CodeOrient is more efficient in retrieving the correct code files within a fewer number of turns which ties directly to the improved latency. 

Thirdly, the similar faithfulness scores between the two models suggest that the multi-layered prompting strategy in CodeOrient successfully mitigates most hallucination issues, and the main performance gains are attributed to the enhanced retrieval and graph generation capabilities provided by the RAG pipeline. Lastly, the use of caching in CodeOrient reduced the latency of repeated queries by an average of 92.2\% and 93.5\% for CodeOrient and the ablation model respectively, which demonstrates the efficiency of caching in improving response times for repeated queries.

\section{Key Findings}
\subsection{Impact of Graph Visualisation and Emergent Inference}
With a Recall@System of 92.50\%, CodeOrient successfully bridges the gap between code retrieval and architectural understanding by generating accurate graph visualisations that capture the relationships between code snippets.

Instances of ``Emergent Inference" was observed, where the system identified imported relationships that were not explicitly retrieved, allowing the system to maintain an accurate graph visualisation even when some relevant files were missed in the initial retrieval step. This is evident where the average Graph Recall (93.61\%) is higher than the average Recall@System (92.50\%), which suggests that the system was able to infer some relationships between code snippets that were not directly retrieved.

\subsection{Effectiveness of Citation Grounding}
The high Faithfulness score of 98.43\% is a direct result of the citation grounding mechanism. By forcing the LLM to provide inline citations for every claim, the system effectively mitigated the hallucination issues mentioned in the literature review. This is particularly important in the context of code retrieval, where inaccurate information can lead to confusion and inefficiency for developers.
\section{Case Studies}
Three case studies based on specific queries from the evaluation dataset are presented to highlight the strengths and limitations of CodeOrient.
\subsection{Case Study 1: ``How does the system resolve and inject dependencies?"}
\label{Case_Study_1}
This query represents a more complex architectural query, where the system only managed to achieve a Recall@System of 50\%. While CodeOrient managed to locate the primary dependency utility file in (\texttt{utils.py}), it was observed that the system failed to retrieve the underlying implementation. 

This highlights a classic ``multi-hop" challenge in code retrieval, where the recursive nature of dependencies are spread across multiple files. This suggests that while semantic search is excellent in retrieving the logically relevant files, there is still a gap in understanding the execution depth of the codebase, which requires a more sophisticated graph traversal to ensure that all relevant files are retrieved. 

This also ties into the graph recall performance, where the system achieved a graph recall of 50\% for this query, which indicates that while the primary nodes were retrieved, some of the secondary nodes (e.g., files containing the underlying implementation) were missed. This case study highlights an area for future improvement in CodeOrient, which is to enhance the multi-hop retrieval capabilities to ensure that all relevant files across multiple levels of dependencies are retrieved effectively.

\subsection{Case Study 2: ``How are the button visual variants defined?"}
This query demonstrates a scenario where even though the system only achieved a Recall@System of 50\%, the generated graph visualisation achieved a graph recall of 100\%. This showcases emergent inference capabilities of the LLM when it comes to understanding import relationships in the code snippets. 

In this case, CodeOrient successfully retrieved the main file defining the button variants (\texttt{button.tsx}), and through the citation grounding and multi-layered prompting, it was able to infer the relevant related file that were not directly retrieved (\texttt{utils.ts}). This is shown in Figure \ref{fig:case_study_2}, where the inferred node is highlighted in red.

This highlights a unique strength of CodeOrient, which is its ability to leverage the reasoning capabilities of LLMs to fill in gaps in retrieval and generate more complete graph visualisations, even when some relevant files are missed in the initial retrieval step. This also suggests that while improving recall is important, enhancing the reasoning and inference capabilities of the system can also significantly contribute to better performance in terms of graph recall and overall user experience.

\begin{figure}[H]
    \centering
    {\small
    \includegraphics[width=5.5in]{fig/fig19.png}
    \caption{\textit{Graph visualisation for Case Study 2}}
    \label{fig:case_study_2}
    }
\end{figure}

\subsection{Case Study 3: ``How is dark mode theme switching implemented?"}
This query serves as a justification for the use of LLM Evaluation Loop in the system. This query required identifying three specific files: \texttt{active-theme.ts}, \texttt{theme-selector.tsx} and \texttt{mode-switcher.tsx}.

CodeOrient achieved a perfect Recall@System of 100\% for this query with a Success@Turn of 3-2-1. This meant that the system refined its search across three turns which led to the successful retrieval of all three files. In contrast, the ablation model stalled at a Recall@System of 67\%. 

Hence, this example highlights the system's ability to identify missing gaps based on the initial retrieval results, and then refining its search queries in the follow-up turns. This resulted in a complete graph visualisation as shown in Figure \ref{fig:case_study_3}, which accurately captured the relationships between the three files.

\begin{figure}[H]
    \centering
    {\small
    \includegraphics[width=5.5in]{fig/fig20.png}
    \caption{\textit{Graph visualisation for Case Study 3}}
    \label{fig:case_study_3}
    }       
\end{figure}

\section{Implications for Developer Onboarding}
The results of this evaluation showcased a reduction of CTD (Clicks to Discovery) from 5.27 with the GitHub Search baseline to 0.07 with CodeOrient. This significant reduction in CTD demonstrates how CodeOrient effectively eliminates the need for developers to manually sift through search results, which is a common pain point in traditional code search. This allows new developers to spend more time understanding the codebase and less time searching for relevant information, which can significantly enhance the onboarding experience and accelerate the time to productivity for new developers.

Futhermore, the high graph recall and faithfulness scores indicate that CodeOrient not only retrieves relevant code snippets but also provides accurate architectural visualisations that can help new developers understand the relationships between different components in the codebase. This can be particularly beneficial for onboarding, as it allows new developers to quickly grasp the overall structure of the codebase and how different files and components interact with each other. Overall, the results suggest that CodeOrient has the potential to significantly improve the developer onboarding experience by providing more efficient and effective code retrieval, as well as accurate architectural visualisations that can aid in understanding the codebase.
\section{Limitations}
\subsection{Technical Limitations}
\subsubsection{Multi-Hop Retrieval Challenge}
The ``multi-hop" retrieval mentioned in \autoref{Case_Study_1} showcases how the system struggles with ``multi-hop" dependency retrieval. As the system is currently limited to a maximum of 3 turns in the evaluation loop, it may not be able to retrieve all relevant files for queries that require deeper levels of dependency traversal. This is a tradeoff between latency and recall, as allowing for more turns could potentially improve recall but would also increase latency. Future work could explore more sophisticated graph traversal techniques or dynamic turn limits based on the complexity of the query to address this limitation.

\subsubsection{Latency}
The average latency of the system is 62.79s (without caching). This is primarily attributed to the RAG pipeline and Evaluation Loop which requires multiple calls to the LLM and vector database before generating the final response. While caching strategies have been implemented to reduce latency for repeated queries, the cold start latency remains a concern for user experience. One solution implemented was to stream the response as it is generated, reducing the perceived latency for users. However, further optimisations in the RAG pipeline and Evaluation Loop are necessary to reduce the overall latency of the system.

\subsubsection{Context Window}
For more complex queries that require retrieving a large number of relevant files, the system may struggle with the context window limitations of the LLM. If the retrieved code snippets exceed the context window, it may lead to incomplete or inaccurate responses. Future work could explore techniques such as chunking or summarisation to ensure that the most relevant information is included within the context window.

\subsection{Threats to Validity}
\subsubsection{Internal Validity}
The stochastic nature of LLMs can lead to variability in the results, which may affect the internal validity of the evaluation. To mitigate this, a temperature of 0.0 was used for all LLM calls to ensure a more deterministic output. 

The scoring of the evaluation metrics, particularly for faithfulness and graph recall, may also introduce subjectivity. To address this, clear guidelines and criteria were established for scoring.
\subsubsection{External Validity}
The evaluation was conducted on a specific set of 30 queries across three modern and well maintained repositories. While these queries were designed to be representative of common developer information needs, they may not cover the full spectrum of queries that developers may have in real-world scenarios. Additionally, the repositories chosen for evaluation may not be representative of all types of codebases, such as legacy codebases or those with less documentation. Therefore, the generalisability of the results to other queries and repositories may be limited.

\subsubsection{Construct Validity}
While Clicks to Discovery (CTD) measures the speed of finding information, it does not necessarily measure the depth of a developer's understanding. However, the high faithfulness and graph recall scores suggest that the information provided by CodeOrient is not only quickly discoverable but also accurate and comprehensive, which can contribute to a deeper understanding of the codebase.




